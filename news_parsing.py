# -*- coding: utf-8 -*-
"""news_parsing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bcc3QhnA98TzU-KDwvCjvUN4rlZ8jfZS

Loading Libraries
"""

import time
from bs4 import BeautifulSoup
import requests
import re
import urllib
import urllib.request as urllib2
import dateutil
from tqdm import tqdm
import _socket
from urllib.error import HTTPError

import pickle
import nltk

from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from wordcloud import WordCloud
import matplotlib.pyplot as plt

"""Words preprocessing """

def text_opening(file_name):
  ''' Returns list of the text 
  
  Parameters
  ----------
  file_name : str
    name of the file to open
  
  '''
  
  with open(file_name, 'rb') as f:
    text = pickle.load(f) #unpickling
  return text


def text_tokenization(text):
  ''' Returns the list with tokenized words written in articles
  
  Parameters
  ----------
  text: list
    text to process
  
  '''

  tok_text = []
  for podtext in text:
    tok_text.append(nltk.word_tokenize(podtext)) #adding tokenized words to the dictionary
  return tok_text

def text_pickling(text, file_name):
  ''' Returns the file .pkl with tokenized words
  
  Parameters
  ----------
  text: list
    text to process
  file_name : str
    name of the new pickled file
  
  '''

  with open(file_name, 'wb') as f:
    pickle.dump(text, f) #pickling text

def text_processing(text):
  ''' Returns the list of words in infinitive from articles without stopwords(like I, and..) and symbols
  
  Parameters
  ----------
  text: list
    text to process
  
  '''

  dictionary = list()
  for podword in text:
    for word in nltk.word_tokenize(podword):
      word = re.sub(r'[^\w\s]', '', word) #deleting the symbols
      word_root = lemmatizer.lemmatize(word) #lemmatisation

      if word_root.lower() not in all_stopwords and word_root != '' and len(word_root) > 2 and word_root.isdigit() == False:
        dictionary.append(word_root) #filling the list
  return dictionary

def common_words(dic, n=2):
  ''' Returns n most common words
  
  Parameters
  ----------
  dic: list
    gets the result of the text_proceccing function
  n: int
    the amount of common words
  '''

  assert n >= 1
  word_set = {} #creating the dictionary

  for word in dic:
    word_set[word] = word_set.get(word, 0) + 1 #counting number of words found

  sorted_keys = sorted(word_set, key=word_set.get, reverse=True) #sorting the words for their nums


  return sorted_keys[0:n] #returning the first n most popular words

def word_cloud(text, num='1'):
  ''' Returns the picture with wordcloud .jpg
  
  Parameters
  ----------
  text: str
    procecced text in str
  num: int
    number of file word_list
  '''

  wordcloud = WordCloud().generate(text) #generating wordcloud
  plt.imshow(wordcloud) 
  plt.axis('off')
  plt.show() #showing wordcloud 
  wordcloud.to_file('cloud'+num+'.jpg') #saving the image with wordcloud

"""New York Times | Parsing"""

apikey='lslAqlRER3GGBe4kGAdhhtsQfmBApy22' # key for getting access to the NYT archieve

def send_request(year, month):

    ''' Returns the data request from NYT archieve in json format 

    Parameters 
    ----------
    year : int 
     the year of the article's publication 

    month : int 
     the month of the article's publication 

    '''

    url=f'https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={apikey}'
    response = requests.get(url).json()
    time.sleep(6) # we sleep for 6 seconds between calls to avoid hitting the per minute rate limit
    return response

hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',
       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
       'Accept-Encoding': 'none',
       'Accept-Language': 'en-US,en;q=0.8',
       'Connection': 'keep-alive'}

def getting_article(link):

  ''' Returns the text of article by the link

    Parameters
    ----------
    link : string
     link to the article

    '''

  try:
    req = urllib2.Request(link, headers=hdr)
    soup = BeautifulSoup(urllib2.urlopen(req))
    txts = soup.find_all('p', attrs={'class' : re.compile("^css-")}) 
    article=[txt.text for txt in txts] 
  except HTTPError:
    errors.append(link)
    pass
  except SocketError as e:
    if e.errno != errno.ECONNRESET:
      raise
    pass
  return article

"""The Economist | Parsing"""

def get_html_archive(url):

  ''' Returns the html code of a web page
  
  Parameters
  ----------
  url : str
   URL-link to the archive
  
  '''
  req = urllib.request.Request(url, headers=hdr)
  page = urllib.request.urlopen(req)
  page_soup = soup(page, 'html.parser')
  return page_soup

def get_links_archive(page_soup):

  ''' Returns all the links to the magazines from a class 'headline-link' with ef0oilz0 and certain data-analytics

  Parameters
  ----------
  page_soup : str
   page_soup from the function get_html_archive

  '''
  resources = page_soup.findAll('a', {'class': 'headline-link'})
  links = []
  for el in resources:
            # if el.find('a').get('data-analytics') in data_analytics:
              links.append('https://www.economist.com/' + str(el.get('href')))
  return links


def get_chunks_archive(page_soup):

  ''' Returns the text of articles from a link

  Parameters
  ----------
  page_soup : str
   page_soup from the function get_html_archive
  
  article__body-text : string
   a link to the article
    
  '''
  resources = page_soup.findAll('p', {'class': 'article__body-text'})
  chunks = []
  for el in resources:
    chunks.append(el.text) #concatenating chunks of the text
  return '. '.join(chunks)