# -*- coding: utf-8 -*-
"""news_parsing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bcc3QhnA98TzU-KDwvCjvUN4rlZ8jfZS

Пелагея
"""

import time
from bs4 import BeautifulSoup
import requests
import re
import urllib
import urllib.request as urllib2
import dateutil
from tqdm import tqdm
import _socket
from urllib.error import HTTPError

import pickle
import nltk

from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from wordcloud import WordCloud
import matplotlib.pyplot as plt

def text_opening(file_name='articles2_march.pkl'):
  ''' returns list of text 
  
  Parameters
  ----------
  file_name : str
    name of the file to open
  
  '''
  
  with open(file_name, 'rb') as f:
    text = pickle.load(f) #unpickling
  return text

class preprocessing:
    def __init__(self):
        self = self 
def text_tokenization(text):
  ''' returns list with tokenized words written in articles
  
  Parameters
  ----------
  text: list
    text to process
  
  '''

  tok_text = []
  for podtext in text:
    tok_text.append(nltk.word_tokenize(podtext)) #adding tokenized words to the dictionary
  return tok_text

def text_pickling(text, file_name='nyt_articles_april.pkl'):
  ''' returns the file .pkl with tokenized words
  
  Parameters
  ----------
  text: list
    text to process
  file_name : str
    name of the new pickled file
  
  '''

  with open(file_name, 'wb') as f:
    pickle.dump(text, f) #pickling text

def text_processing(text):
  ''' returns list of words in infinitive from the articles without stopwords(like I, and..) and symbols
  
  Parameters
  ----------
  text: list
    text to process
  
  '''

  dictionary = list()
  for podword in text:
    for word in nltk.word_tokenize(podword):
      word = re.sub(r'[^\w\s]', '', word) #deleting the symbols
      word_root = lemmatizer.lemmatize(word) #lemmatisation

      if word_root.lower() not in all_stopwords and word_root != '' and len(word_root) > 2 and word_root.isdigit() == False:
        dictionary.append(word_root) #filling the list
  return dictionary

def common_words(dic, n=2):
  ''' returns n most common words
  
  Parameters
  ----------
  dic: list
    gets the result of the text_proceccing function
  n: int
    the amount of common words
  '''

  assert n >= 1
  word_set = {} #creating the dictionary

  for word in dic:
    word_set[word] = word_set.get(word, 0) + 1 #counting number of words found

  sorted_keys = sorted(word_set, key=word_set.get, reverse=True) #sorting the words for their nums


  return sorted_keys[0:n] #returning the first n most popular words

def word_cloud(text, num='1'):
  ''' returns the picture with wordcloud .jpg
  
  Parameters
  ----------
  text: str
    procecced text in str
  num: int
    number of file word_list
  '''

  wordcloud = WordCloud().generate(text) #generating wordcloud
  plt.imshow(wordcloud) 
  plt.axis('off')
  plt.show() #showing wordcloud 
  wordcloud.to_file('cloud'+num+'.jpg') #saving the image with wordcloud

apikey='lslAqlRER3GGBe4kGAdhhtsQfmBApy22' # key for getting access to the NYT archieve

def send_request(year, month):

    ''' Returns the data request from NYT archieve in json format 

    Parameters 
    ----------
    year : int 
     the year of the necessary article's publication 

    month : int 
     the month of the necessary article's publication 

    '''

    url=f'https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={apikey}'
    response = requests.get(url).json()
    time.sleep(6) # we sleep for 6 seconds between calls to avoid hitting the per minute rate limit
    return response

hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',
       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
       'Accept-Encoding': 'none',
       'Accept-Language': 'en-US,en;q=0.8',
       'Connection': 'keep-alive'}

def getting_article(link):

  ''' Returns the text of article by link

    Parameters
    ----------
    link : string
     link to the article

    '''

  try:
    req = urllib2.Request(link, headers=hdr)
    soup = BeautifulSoup(urllib2.urlopen(req))
    txts = soup.find_all('p', attrs={'class' : re.compile("^css-")}) 
    article=[txt.text for txt in txts] 
  except HTTPError:
    errors.append(link)
    pass
  except SocketError as e:
    if e.errno != errno.ECONNRESET:
      raise
    pass
  return article